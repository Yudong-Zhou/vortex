# Vortex GPU DMA Engine 设计与实现报告

## 一、DMA工作原理

### 1.1 什么是DMA

DMA（Direct Memory Access，直接内存访问）是一种允许硬件子系统独立于CPU直接访问主存的技术。在GPU架构中，DMA引擎可以在全局内存（Global Memory）和本地内存（Local Memory/Shared Memory）之间高效传输数据，而无需CPU/GPU核心的直接参与。

### 1.2 DMA vs 传统Load/Store

| 特性 | 传统Load/Store | DMA传输 |
|------|---------------|---------|
| 执行方式 | 逐元素访问 | 批量块传输 |
| Cache参与 | 经过Cache层级 | 可绕过Cache |
| CPU/GPU参与 | 每次访问都需要 | 仅配置和启动 |
| 适用场景 | 随机访问、小数据 | 大块连续数据 |
| 带宽利用 | 依赖Cache | 可达峰值带宽 |

### 1.3 DMA传输流程

```
1. 配置阶段：设置源地址、目标地址、传输大小、传输方向
2. 启动阶段：触发DMA传输，获取传输ID
3. 传输阶段：DMA引擎自主完成数据搬运（与计算可并行）
4. 等待阶段：通过ID查询/等待传输完成
5. 使用阶段：数据就绪，可被计算单元使用
```

---

## 二、DMA Engine架构设计

### 2.1 整体架构

DMA Engine作为Socket级别的共享资源，为同一Socket下的所有Core提供DMA服务：

```
┌─────────────────────────────────────────────────────┐
│                      Socket                          │
│  ┌──────┐  ┌──────┐  ┌──────┐  ┌──────┐            │
│  │Core 0│  │Core 1│  │Core 2│  │Core 3│            │
│  └──┬───┘  └──┬───┘  └──┬───┘  └──┬───┘            │
│     │         │         │         │                 │
│     └─────────┴────┬────┴─────────┘                 │
│                    ▼                                 │
│           ┌────────────────┐                        │
│           │   DMA Engine   │◄──► Global Memory      │
│           │  (共享资源)     │◄──► Local Memory       │
│           └────────────────┘                        │
└─────────────────────────────────────────────────────┘
```

### 2.2 指令设计

采用多指令序列设计，使用RISC-V CUSTOM-2扩展指令空间（opcode=0x2B），通过I-type格式编码：

| 指令 | funct3 | 功能 | 说明 |
|------|--------|------|------|
| DMA_SET_DST | 0x1 | 设置目标地址 | rd = 0, rs1 = 地址寄存器 |
| DMA_SET_SRC | 0x2 | 设置源地址 | rd = 0, rs1 = 地址寄存器 |
| DMA_SET_SIZE | 0x3 | 设置传输大小 | rd = 0, rs1 = 大小寄存器 |
| DMA_TRIGGER | 0x0 | 触发传输 | rd = ID输出, imm[0] = 方向 |
| DMA_WAIT | 0x4 | 等待完成 | rs1 = ID寄存器 |

**设计优势**：
- 使用I-type格式简化解码逻辑
- 分离配置和触发，支持灵活的参数设置
- 返回唯一ID，支持多个并发DMA追踪

### 2.3 流水线集成

DMA指令通过SFU（Special Function Unit）执行：

```
┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐
│ Fetch   │───►│ Decode  │───►│ Execute │───►│  SFU    │
└─────────┘    └─────────┘    └─────────┘    └─────────┘
                                   │              │
                                   ▼              ▼
                           配置DMA参数      处理WAIT指令
                           触发DMA传输      查询完成状态
                                              │
                                              ▼
                                    ┌─────────────────┐
                                    │   DMA Engine    │
                                    └─────────────────┘
```

### 2.4 Warp同步机制

DMA WAIT指令采用fetch_stall机制：
1. 执行WAIT时，设置warp的fetch_stall标志
2. 被stall的warp暂停取指，让出资源给其他warp
3. SFU每周期轮询DMA完成状态
4. 完成后清除stall标志，恢复warp执行

---

## 三、并行DMA优化

### 3.1 问题分析

初始实现中，DMA Engine采用单通道串行处理：

```
时间 ──────────────────────────────────────────────►
     │ DMA 1 │ DMA 2 │ DMA 3 │ DMA 4 │
     ├───────┼───────┼───────┼───────┤
     启动+传输  启动+传输  启动+传输  启动+传输
```

**瓶颈**：即使多个DMA请求同时提交，也只能串行处理，无法充分利用内存带宽。

### 3.2 多通道并行设计

引入多通道架构，支持多个DMA传输同时进行：

```
时间 ──────────────────────────────────────────────►
Channel 0: │ DMA 1 │         │ DMA 5 │
Channel 1: │ DMA 2 │         │ DMA 6 │
Channel 2: │ DMA 3 │         │ DMA 7 │
Channel 3: │ DMA 4 │         │ DMA 8 │
           └───────┘
           同时处理4个DMA!
```

**实现要点**：
- 请求队列统一管理所有待处理DMA
- tick()每周期为空闲通道分配新请求
- 各通道独立处理传输进度
- 完成的传输移入完成队列供查询

### 3.3 Kernel端并行优化

为充分利用多通道DMA，需要让多个线程**同时**发起DMA请求：

**优化前**（串行提交）：
```
Thread 0: [DMA配置1] → [DMA配置2] → [DMA配置3] → [DMA配置4]
其他线程: 空闲等待
```

**优化后**（并行提交）：
```
Thread 0: [DMA配置1]  ──────────────────────►
Thread 1: [DMA配置2]  ──────────────────────►
Thread 2: [DMA配置3]  ──────────────────────►
Thread 3: [DMA配置4]  ──────────────────────►
          └─ 同时提交到DMA Engine ─┘
```

**策略**：每个线程负责一个特定的数据块（如tile的一行），同时发起DMA请求。

---

## 四、性能实验与分析

### 4.1 测试配置

| 参数 | 值 |
|------|-----|
| 测试程序 | SGEMM（矩阵乘法） |
| 矩阵大小 | 16×16, 32×32 |
| Tile大小 | 4×4 |
| DMA通道数 | 4 |
| DMA带宽 | 64 bytes/cycle |
| 启动延迟 | 10 cycles |

### 4.2 性能对比结果

| 版本 | n=16 Cycles | n=16 IPC | 说明 |
|------|-------------|----------|------|
| 原版sgemm2 | 92,731 | 1.042 | 直接load/store |
| DMA串行版 | 232,815 | 0.471 | 单线程发DMA |
| **DMA并行版** | **125,950** | **0.867** | 多线程并行发DMA |

### 4.3 结果分析

**并行优化效果**：
- 并行DMA比串行DMA快 **1.85倍**（232,815 → 125,950 cycles）
- 多通道设计成功发挥作用

**DMA vs 原版对比**：
- 原版sgemm2仍然更快（92,731 vs 125,950 cycles）
- 原因分析：
  1. **小数据量**：每次DMA仅传输16字节，启动延迟占比过高
  2. **Cache优势**：原版利用Cache，命中延迟仅1-2周期
  3. **指令开销**：每个DMA需要4条配置指令+1条等待指令

### 4.4 DMA适用场景总结

| 场景 | DMA优势 | 原因 |
|------|---------|------|
| 大块连续数据 | ✅ 高 | 启动延迟摊销，带宽充分利用 |
| 流式数据处理 | ✅ 高 | 绕过Cache，避免污染 |
| 计算密集型（Cache友好） | ❌ 低 | Cache命中率高时延迟更低 |
| 小数据随机访问 | ❌ 低 | DMA启动开销无法摊销 |

---

## 五、设计经验总结

1. **模块化设计**：DMA Engine作为独立模块，通过清晰的接口与流水线集成，便于维护和扩展。

2. **多通道并行**：通过增加硬件资源（多通道）提升吞吐量，是处理I/O密集型任务的有效方法。

3. **软硬件协同**：硬件提供并行能力，软件（Kernel）需要相应优化才能充分利用。单纯的硬件优化如果没有软件配合，效果有限。

4. **场景适配**：没有万能的优化方案。DMA适合特定场景，在Cache友好的计算中反而可能引入额外开销。

5. **性能建模**：在设计阶段分析启动延迟、传输带宽、数据粒度等参数，有助于预判优化效果。

---

## 六、未来改进方向

1. **双缓冲机制**：实现DMA传输与计算的重叠（Prefetch），隐藏传输延迟。

2. **2D DMA支持**：支持带stride的2D数据传输，适配矩阵运算中的非连续数据访问模式。

3. **自适应调度**：根据数据大小自动选择DMA或Cache路径。

4. **DMA压缩**：对传输数据进行压缩，减少带宽需求。

---

## 附录：关键文件清单

| 文件路径 | 功能说明 |
|---------|---------|
| `sim/simx/dma_engine.h` | DMA Engine头文件，定义接口和数据结构 |
| `sim/simx/dma_engine.cpp` | DMA Engine实现，包含多通道并行逻辑 |
| `sim/simx/decode.cpp` | 指令解码，处理DMA指令（opcode=0x2B） |
| `sim/simx/execute.cpp` | 指令执行，配置DMA参数 |
| `sim/simx/func_unit.cpp` | SFU功能单元，处理DMA WAIT |
| `kernel/include/vx_intrinsics.h` | Kernel端DMA内联函数 |
| `tests/regression/dma_test/` | DMA功能测试 |
| `tests/regression/sgemm2_dma/` | SGEMM性能对比测试 |

